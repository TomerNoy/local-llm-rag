# Local LLM RAG

A **local RAG pipeline** for querying your personal documents in natural language. Everything runs on your machine: no cloud, no API keys for your data. Built with **Hebrew and multilingual support** in mind (embeddings and LLM can handle Hebrew, English, and 50+ languages).

---

## Project purpose

- **RAG over your data**: Drop files (PDF, TXT, DOCX, images, etc.) into the folder you configure as the watched directory (see `config.json`); the pipeline converts them to text, chunks and embeds them, and stores them in a vector database. You then ask questions in natural language and get answers grounded in your documents.
- **Local-first**: Ingestion, embeddings, and storage are fully local. For answers, you use a local LLM (e.g. via [LM Studio](https://lmstudio.ai/)) so your content never leaves your machine.
- **Hebrew support**: The embedding model is multilingual (e.g. `paraphrase-multilingual-MiniLM-L12-v2`), so Hebrew (and other languages) are supported for both indexing and semantic search.

---

## High-level architecture

```
┌─────────────────┐     ┌──────────────────┐     ┌─────────────────┐     ┌──────────────────┐
│  Ingestion      │     │  Storage         │     │  Query /        │     │  LLM generation  │
│                 │     │                  │     │  retrieval      │     │                  │
│  watched folder │────►│  storage/        │────►│  Semantic       │────►│  Local LLM       │
│  (configurable) │     │  md-content/     │     │  search +       │     │  (LM Studio,     │
│  → watch → MD   │     │  + lancedb/      │     │  metadata       │     │   OpenAI-compat) │
│  → ingest       │     │  (vector DB)     │     │  (PydanticAI)   │     │                  │
└─────────────────┘     └──────────────────┘     └─────────────────┘     └──────────────────┘
```

1. **Ingestion**
   - **Watch**: Monitors the configured watched folder (path in `config.json`, default `watched-dir/`); converts supported files (PDF, TXT, DOCX, RTF, HTML, images with OCR) to markdown into the configured md-content directory.
   - **Ingest**: Chunks markdown, computes embeddings with a multilingual model, and writes chunks + vectors into LanceDB.

2. **Storage**
   - **Markdown**: Intermediate text files under `storage/md-content/` (by source path).
   - **Vector DB**: [LanceDB](https://lancedb.com/) under `storage/lancedb/` — stores document chunks, embeddings, and metadata (file name, type, tags, timestamps).

3. **Query / retrieval**
   - **Query service**: PydanticAI agent with tools: semantic search, list files, filter by tag/type/date, get chunk content. Same logic powers both CLI and the web UI.
   - **Retrieval**: User question → same embedding model → vector search in LanceDB → relevant chunks passed to the LLM.

4. **LLM generation**
   - **Local LLM**: Answers are generated by a model you run locally (e.g. in LM Studio), via an OpenAI-compatible API. No document content is sent to the cloud.

---

## Tech stack

| Layer           | Technology |
|----------------|------------|
| **Language**   | Python 3 |
| **Embeddings** | [sentence-transformers](https://www.sbert.net/) — `paraphrase-multilingual-MiniLM-L12-v2` (384 dims, 50+ languages including Hebrew) |
| **Vector DB**  | [LanceDB](https://lancedb.com/) (embedded, no separate server) |
| **Chunking**   | [LangChain](https://python.langchain.com/) `MarkdownTextSplitter` |
| **RAG / agent**| [PydanticAI](https://docs.pydantic.ai/latest/) — tools for search, file listing, content retrieval |
| **LLM**        | Any OpenAI-compatible API (default: LM Studio at `http://127.0.0.1:1234/v1`) |
| **Web UI**     | [Gradio](https://gradio.app/) chat interface |
| **Pipeline**   | Single orchestrator script (`run_pipeline.py`) — stdlib only; invokes watch + ingest via per-service venvs |
| **PDF → markdown** | [Docling](https://github.com/DS4SD/docling) (digital + scanned/OCR) in `services/watch/pdf_converter` (doclin-venv); used by default for PDFs when the venv is set up. |

---

## Example outputs

### CLI (query service)

```bash
python services/query/query.py "summarize the main points from the budget report"
# → Model retrieves relevant chunks from your indexed documents and returns a summary.
```

```bash
python services/query/query.py "מה המסקנות העיקריות בדו\"ח שהעליתי?"   # What are the main conclusions in the report I uploaded?
# → Answer grounded in the retrieved Hebrew (or multilingual) content from your documents.
```

### Web UI (Gradio)

The **front** is a simple chat UI:

- **Title**: "RAG Query"
- **Description**: Shows how many files are indexed (e.g. "Ask questions about your documents. (12 files indexed.)")
- **Input**: Single text box; you type a question and get one answer per message (no multi-turn conversation history sent to the agent).

If the DB is empty or the LLM server is not reachable, the UI shows a short error message instead of calling the model.

---

## Quick start

### Prerequisites

- **Python 3** and **[uv](https://docs.astral.sh/uv/)** (for venvs and installs).
- **LM Studio** (or another OpenAI-compatible server) with a model loaded — required for generating answers.
- **Optional** (for full format support): Pandoc, Tesseract, Poppler (e.g. on macOS: `brew install pandoc tesseract poppler tesseract-lang`).

### Installation & usage

From the **project root**:

```bash
# 1. Create all venvs and install dependencies (idempotent)
make setup

# 2. Run the pipeline: initial sync + ingest, then watch for changes and re-ingest on change
make pipeline
```

Leave the pipeline running (or run it whenever you add/change files). In another terminal:

```bash
# 3. Start the Gradio UI to ask questions (uses front/.venv if present)
make front
```

Then open the URL Gradio prints (e.g. `http://127.0.0.1:7860`), put files in your watched folder (path in `config.json`), and once ingest has run, ask questions in the chat.

### Optional: LLM endpoint

Default is LM Studio at `http://127.0.0.1:1234/v1`. Override with:

```bash
export OPENAI_BASE_URL=http://127.0.0.1:1234/v1
export OPENAI_API_KEY=lm-studio
```

### Optional: CLI-only query (no UI)

```bash
cd services/query && .venv/bin/python query.py "your question"
# or interactive:
cd services/query && .venv/bin/python query.py --interactive
```

(Ensure `services/query` dependencies are installed, e.g. via a venv there or by using the front venv and running from root.)

---

## Why this matters

- **Privacy**: Your documents never leave your machine. Ingestion and embedding run locally; the only “API” is your local LLM.
- **Local inference**: No dependency on OpenAI/Anthropic/Google for the RAG answers; you choose the model (e.g. small and fast or larger and more capable).
- **Control**: No vendor lock-in for the pipeline; you can switch embedding model or LLM backend while keeping the same workflow.
- **Hebrew and multilingual**: First-class support for non-English content in both indexing and querying.

---

## Project structure (reference)

```
local-llm-rag/
├── config.json              # watched_dir, md_content_dir, lancedb_dir, ingest_debounce_seconds
├── run_pipeline.py          # Orchestrator: sync → ingest → watch loop
├── Makefile                 # setup | pipeline | front
├── watched-dir/             # Drop files here (gitignored)
├── storage/                 # Runtime data (gitignored)
│   ├── md-content/          # Converted markdown
│   └── lancedb/             # Vector DB
├── services/
│   ├── watch/               # File watcher + conversion to markdown
│   │   ├── .venv/
│   │   ├── watch.py, requirements.txt
│   │   └── pdf_converter/   # Docling-based PDF→MD (doclin-venv)
│   ├── ingest/              # Chunk + embed + write to LanceDB
│   │   ├── .venv/
│   │   └── ingest.py, requirements.txt
│   └── query/               # PydanticAI RAG agent (semantic search + LLM)
│       └── query.py, requirements.txt
└── front/                   # Gradio chat UI
    ├── .venv/
    ├── start.py, requirements.txt
```

Each service uses its own virtual environment (created by `make setup` with `uv`). The pipeline script calls into `services/watch` and `services/ingest` via their `.venv` interpreters.
